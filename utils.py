from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from typing import Optional, List
import os
from datetime import date
from langchain_community.document_loaders import PyPDFLoader, UnstructuredMarkdownLoader
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
# --- å¯¼å…¥æ–°çš„ prompt ---
from prompts import jd_analysis_prompt, jd_analysis_prompt_legacy

# === å…¨å±€RAGé“¾ç¼“å­˜ ===
_rag_chain_cache = None
_retriever_cache = None

def load_knowledge_base(kb_path: str) -> List[Document]:
    """
    åŠ è½½çŸ¥è¯†åº“æ–‡ä»¶ï¼ˆæ”¯æŒ .pdf å’Œ .mdï¼‰
    
    Args:
        kb_path: çŸ¥è¯†åº“ç›®å½•è·¯å¾„
        
    Returns:
        åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨
    """
    documents = []
    if not os.path.isdir(kb_path):
        print(f"è­¦å‘Š: çŸ¥è¯†åº“ç›®å½• '{kb_path}' ä¸å­˜åœ¨ã€‚")
        return documents

    for filename in os.listdir(kb_path):
        file_path = os.path.join(kb_path, filename)
        if filename.endswith(".pdf"):
            loader = PyPDFLoader(file_path)
            documents.extend(loader.load())
        elif filename.endswith(".md"):
            loader = UnstructuredMarkdownLoader(file_path)
            documents.extend(loader.load())
    
    print(f"æˆåŠŸä» '{kb_path}' åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µã€‚")
    return documents

def create_rag_chain(prompt_template: ChatPromptTemplate, temperature: float = 0.7):
    """
    åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªå®Œæ•´çš„ RAG (Retrieval-Augmented Generation) é“¾ã€‚
    ä½¿ç”¨å…¨å±€ç¼“å­˜é¿å…é‡å¤åˆ›å»ºï¼Œè§£å†³Agentæ¨¡å¼ä¸‹çš„APIé™æµé—®é¢˜ã€‚
    """
    global _rag_chain_cache, _retriever_cache
    
    # æ£€æŸ¥ç¼“å­˜
    if _rag_chain_cache is not None and _retriever_cache is not None:
        print("âœ… ä½¿ç”¨ç¼“å­˜çš„RAGé“¾ï¼Œé¿å…é‡å¤å‘é‡åŒ–")
        return _rag_chain_cache, _retriever_cache
    
    print("ğŸ”„ é¦–æ¬¡åˆ›å»ºRAGé“¾...")
    
    # 1. åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£
    kb_path = os.path.join(os.path.dirname(__file__), 'knowledge_base')
    documents = load_knowledge_base(kb_path)
    
    if not documents:
        print("çŸ¥è¯†åº“ä¸ºç©ºï¼Œæ— æ³•åˆ›å»º RAG é“¾ã€‚")
        return None, None # è¿”å› None è¡¨ç¤ºæ— æ³•åˆ›å»º

    # 2. æ–‡æœ¬åˆ†å‰²
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(documents)

    # 3. åˆ›å»º Embedding æ¨¡å‹
    # ä½¿ç”¨ OpenAI ä¸“é—¨çš„ embedding æ¨¡å‹ï¼Œæ•ˆæœå’Œå…¼å®¹æ€§æ›´å¥½
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ° OpenAI API Keyï¼Œæˆ‘ä»¬å¯ä»¥æ‰“å°ä¸€ä¸ªæ›´å‹å¥½çš„é”™è¯¯æç¤ºï¼Œè€Œä¸æ˜¯è®©ç¨‹åºå´©æºƒ
        print("é”™è¯¯: æœªæ‰¾åˆ° OPENAI_API_KEY ç¯å¢ƒå˜é‡ã€‚RAG åŠŸèƒ½éœ€è¦æ­¤å¯†é’¥ã€‚")
        print("å°†å›é€€åˆ°æ— çŸ¥è¯†åº“çš„æ™®é€šåˆ†ææ¨¡å¼ã€‚")
        return None, None # è¿”å› None è¡¨ç¤ºæ— æ³•åˆ›å»º RAG é“¾

    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=openai_api_key
    )

    # 4. åˆ›å»ºå‘é‡æ•°æ®åº“å’Œæ£€ç´¢å™¨
    # é‡‡ç”¨åˆ†æ‰¹å¤„ç†çš„æ–¹å¼ï¼Œé˜²æ­¢ä¸€æ¬¡æ€§å‘é€è¿‡å¤štokenå¯¼è‡´APIé”™è¯¯
    if not split_docs:
        print("è­¦å‘Š: çŸ¥è¯†åº“ä¸ºç©ºæˆ–åˆ†å‰²åæ— å†…å®¹ã€‚")
        return None, None
        
    print("æ­£åœ¨åˆ†æ‰¹ä¸ºçŸ¥è¯†åº“æ–‡æ¡£åˆ›å»ºå‘é‡ç´¢å¼•...")
    # å…ˆç”¨ç¬¬ä¸€ä¸ªæ–‡æ¡£å—åˆå§‹åŒ– FAISS ç´¢å¼•
    vector_store = FAISS.from_documents([split_docs[0]], embeddings)
    
    # å®šä¹‰ä¸€ä¸ªåˆç†çš„æ‰¹å¤„ç†å¤§å°
    batch_size = 32 
    
    # å¾ªç¯å¤„ç†å‰©ä½™çš„æ–‡æ¡£å—
    for i in range(1, len(split_docs), batch_size):
        batch = split_docs[i:i + batch_size]
        if batch:
            vector_store.add_documents(batch)
            print(f"  - å·²å¤„ç†æ‰¹æ¬¡ {i // batch_size + 1}")
    
    print("å‘é‡ç´¢å¼•åˆ›å»ºå®Œæˆï¼")
    retriever = vector_store.as_retriever()

    # 5. åˆ›å»ºå¹¶è¿”å› RAG é“¾
    llm = init_moonshot_llm(temperature)
    
    # è¿™ä¸ªé“¾è´Ÿè´£å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£"å¡å…¥"æç¤ºè¯
    question_answer_chain = create_stuff_documents_chain(llm, prompt_template)
    
    # è¿™ä¸ªé“¾è´Ÿè´£"æ£€ç´¢"->"å¡å…¥"->"ç”Ÿæˆ"çš„å®Œæ•´æµç¨‹
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)
    
    # ç¼“å­˜ç»“æœ
    _rag_chain_cache = rag_chain
    _retriever_cache = retriever
    
    print("âœ… RAGé“¾åˆ›å»ºå®Œæˆå¹¶å·²ç¼“å­˜")
    return rag_chain, retriever # åŒæ—¶è¿”å›æ£€ç´¢å™¨ï¼Œæ–¹ä¾¿è°ƒè¯•

def init_moonshot_llm(temperature: float = 0.7) -> ChatOpenAI:
    """åˆå§‹åŒ– Moonshot API å®¢æˆ·ç«¯"""
    api_key = os.getenv("MOONSHOT_API_KEY")
    if not api_key:
        raise ValueError("è¯·è®¾ç½® MOONSHOT_API_KEY ç¯å¢ƒå˜é‡")
    
    return ChatOpenAI(
        temperature=temperature,
        model_name="moonshot-v1-8k",
        openai_api_key=api_key,
        openai_api_base="https://api.moonshot.cn/v1",
        max_tokens=4096,  # æå‡æœ€å¤§è¾“å‡ºé•¿åº¦ï¼Œè§£å†³å†…å®¹æˆªæ–­é—®é¢˜
        request_timeout=120,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°2åˆ†é’Ÿ
        max_retries=5  # å¢åŠ é‡è¯•æ¬¡æ•°
    )

def analyze_job_description(
    jd_content: str,
    resume_content: str,
    temperature: float = 0.7
) -> tuple[Optional[str], Optional[str]]:
    """
    åˆ†æèŒä½æè¿°å’Œç®€å†å†…å®¹
    (å·²æ›´æ–°ä¸º LangChain æœ€æ–°è¯­æ³•ï¼Œå¹¶é›†æˆ RAG åŠŸèƒ½)
    """
    try:
        # å‡å°‘å»¶è¿Ÿï¼Œå› ä¸ºç°åœ¨æœ‰ç¼“å­˜æœºåˆ¶
        import time
        time.sleep(1)  # ä»5ç§’å‡å°‘åˆ°1ç§’
        
        # 1. å°è¯•åˆ›å»º RAG é“¾ï¼ˆç°åœ¨æœ‰ç¼“å­˜ï¼Œä¸ä¼šé‡å¤å‘é‡åŒ–ï¼‰
        rag_chain, retriever = create_rag_chain(jd_analysis_prompt, temperature)
        
        # 2. å¦‚æœ RAG é“¾åˆ›å»ºæˆåŠŸï¼Œåˆ™ä½¿ç”¨ RAG æµç¨‹
        if rag_chain:
            print("æ­£åœ¨ä½¿ç”¨ RAG æµç¨‹è¿›è¡Œåˆ†æ...")
            # å°†JDä½œä¸ºæ ¸å¿ƒè¾“å…¥ï¼Œç”¨äºæ£€ç´¢çŸ¥è¯†åº“
            input_dict = {
                "input": jd_content, 
                "jd_content": jd_content,
                "resume_content": resume_content
            }
            response = rag_chain.invoke(input_dict)
            return response.get("answer"), None
            
        # 3. å¦‚æœ RAG é“¾åˆ›å»ºå¤±è´¥ï¼ˆå¦‚çŸ¥è¯†åº“ä¸ºç©ºï¼‰ï¼Œåˆ™å›é€€åˆ°æ™®é€šæµç¨‹
        else:
            print("çŸ¥è¯†åº“ä¸ºç©ºæˆ–åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šåˆ†ææµç¨‹...")
            llm = init_moonshot_llm(temperature)
            chain = jd_analysis_prompt_legacy | llm
            
            response = chain.invoke({
                "jd_content": jd_content,
                "resume_content": resume_content
            })
            
            return response.content, None
        
    except Exception as e:
        error_msg = f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
        if "429" in str(e) or "rate_limit" in str(e):
            error_msg += "\n\nğŸ’¡ æç¤ºï¼šAPIé™æµï¼Œè¯·ç­‰å¾…1-2åˆ†é’Ÿåé‡è¯•"
        return None, error_msg

def generate_interview_schedule(
    prompt_template: PromptTemplate,
    interview_date: date,
    jd_analysis_result: str,  # æ–°å¢å‚æ•°
    temperature: float = 0.7
) -> tuple[Optional[str], Optional[str]]:
    """æ ¹æ®é¢è¯•æ—¥æœŸå’ŒJDåˆ†ææŠ¥å‘Šï¼Œç”Ÿæˆä¸ªæ€§åŒ–çš„å‡†å¤‡è®¡åˆ’"""
    try:
        today = date.today()
        days_diff = (interview_date - today).days
        
        if days_diff < 0:
            return None, "é¢è¯•æ—¥æœŸä¸èƒ½æ—©äºä»Šå¤©ã€‚"
        if days_diff == 0:
            return "å°±æ˜¯ä»Šå¤©ï¼ç¥ä½ é¢è¯•é¡ºåˆ©ï¼Œå‘æŒ¥å‡ºæœ€ä½³æ°´å¹³ï¼", None

        llm = init_moonshot_llm(temperature)
        chain = prompt_template | llm

        weekdays = ["æ˜ŸæœŸä¸€", "æ˜ŸæœŸäºŒ", "æ˜ŸæœŸä¸‰", "æ˜ŸæœŸå››", "æ˜ŸæœŸäº”", "æ˜ŸæœŸå…­", "æ˜ŸæœŸæ—¥"]
        
        response = chain.invoke({
            "jd_analysis_result": jd_analysis_result,  # ä¼ å…¥æ–°å‚æ•°
            "days_to_interview": days_diff + 1,
            "today_date": today.strftime("%Yå¹´%mæœˆ%dæ—¥") + " " + weekdays[today.weekday()],
            "interview_date": interview_date.strftime("%Yå¹´%mæœˆ%dæ—¥") + " " + weekdays[interview_date.weekday()]
        })

        return response.content, None

    except Exception as e:
        error_msg = f"ä»»åŠ¡ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
        return None, error_msg

def validate_inputs(jd_content: str, resume_content: str) -> tuple[bool, str]:
    """éªŒè¯è¾“å…¥å†…å®¹
    
    Args:
        jd_content: èŒä½æè¿°å†…å®¹
        resume_content: ç®€å†å†…å®¹
        
    Returns:
        tuple[bool, str]: (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
    """
    if not jd_content.strip():
        return False, "è¯·å¡«å†™èŒä½æè¿°(JD)å†…å®¹"
    
    if not resume_content.strip():
        return False, "è¯·å¡«å†™ç®€å†å†…å®¹"
    
    if len(jd_content) < 50:
        return False, "èŒä½æè¿°å†…å®¹è¿‡çŸ­ï¼Œè¯·æä¾›æ›´è¯¦ç»†çš„ä¿¡æ¯"
    
    if len(resume_content) < 50:
        return False, "ç®€å†å†…å®¹è¿‡çŸ­ï¼Œè¯·æä¾›æ›´è¯¦ç»†çš„ä¿¡æ¯"
    
    return True, "" 